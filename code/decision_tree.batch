> args <- commandArgs(trailingOnly = TRUE)
> print(args)
[1] "'gini'" "10"     "30"    
> 
> library(rpart) 
> library(mda)
Loading required package: class
Loaded mda 0.4-4

> 
> set.seed(123)
> k <- 10 # For 10-fold cross-validation
> file=file.choose()
> dataset <- read.csv(file, na.strings=c(".", "NA", "", "?"), strip.white=TRUE, encoding="UTF-8")
> file_name <- basename(file)
> ############################# Partition ##############################
>  
>   n <- nrow(dataset)
>   fold <- rep(0,n)
>   c <- 0
>   for (i in 1:(n %/% k)) {
+     for (j in 1:k) {
+       fold[c] <- j
+       c <- c + 1
+     }
+   }
>   fold <- fold[sample(1:length(fold), length(fold), replace=F)]
>   
>   # Perform k-fold Cross Validation  
>   m.error <- 100
>   bestmodel <- ""
>   
>   for(i in 1:k) {
+     cv.index = which(fold == i)
+     train.index = setdiff(1:length(fold),cv.index)
+     
+     tree = rpart(bug ~., dataset[train.index,], method="class", parms=list(split=args[1]), control=rpart.control(minsplit=args[2],minbucket=args[3],cp=0))
+     pred <- predict(tree, dataset[cv.index,], type="class")
+     cm <- confusion(pred, factor(dataset[cv.index,"bug"], levels=c(0,1)))
+     c.error <- as.numeric(as.character(attr(cm, "error")))
+     
+     if(c.error < m.error) {
+       m.error <- c.error
+       bestmodel <- tree
+     }
+   }
> 
> bestmodel
n= 1358 

node), split, n, loss, yval, (yprob)
      * denotes terminal node

  1) root 1358 661 0 (0.51325479 0.48674521)  
    2) rfc< 45.06106 849 267 0 (0.68551237 0.31448763)  
      4) ce< 1.061254 226  17 0 (0.92477876 0.07522124) *
      5) ce>=1.061254 623 250 0 (0.59871589 0.40128411)  
       10) lcom< 0.01463686 173  37 0 (0.78612717 0.21387283)  
         20) npm< 5.194931 112  14 0 (0.87500000 0.12500000) *
         21) npm>=5.194931 61  23 0 (0.62295082 0.37704918)  
           42) cbo>=8.571526 30   6 0 (0.80000000 0.20000000) *
           43) cbo< 8.571526 31  14 1 (0.45161290 0.54838710) *
       11) lcom>=0.01463686 450 213 0 (0.52666667 0.47333333)  
         22) moa>=0.9832697 92  22 0 (0.76086957 0.23913043) *
         23) moa< 0.9832697 358 167 1 (0.46648045 0.53351955)  
           46) moa< 0.001031053 321 154 0 (0.52024922 0.47975078)  
             92) dit>=1.5 216  84 0 (0.61111111 0.38888889)  
              184) cbo>=4.951786 156  49 0 (0.68589744 0.31410256)  
                368) lcom< 4.475731 96  20 0 (0.79166667 0.20833333) *
                369) lcom>=4.475731 60  29 0 (0.51666667 0.48333333)  
                  738) lcom>=14.14753 30   8 0 (0.73333333 0.26666667) *
                  739) lcom< 14.14753 30   9 1 (0.30000000 0.70000000) *
              185) cbo< 4.951786 60  25 1 (0.41666667 0.58333333) *
             93) dit< 1.5 105  35 1 (0.33333333 0.66666667) *
           47) moa>=0.001031053 37   0 1 (0.00000000 1.00000000) *
    3) rfc>=45.06106 509 115 1 (0.22593320 0.77406680)  
      6) dam< 0.6859609 79  37 1 (0.46835443 0.53164557)  
       12) cbo>=10.96938 49  21 0 (0.57142857 0.42857143) *
       13) cbo< 10.96938 30   9 1 (0.30000000 0.70000000) *
      7) dam>=0.6859609 430  78 1 (0.18139535 0.81860465)  
       14) cbo< 7.186921 30  14 0 (0.53333333 0.46666667) *
       15) cbo>=7.186921 400  62 1 (0.15500000 0.84500000) *
> modelfile=paste(paste("../models/",file_name, sep=""), ".rda", sep="")
> modelfile
[1] "../models/ant-training.csv.rda"
> save(bestmodel, file=modelfile)
> ################# End of Pre-processing and partition ################
> 
> 
